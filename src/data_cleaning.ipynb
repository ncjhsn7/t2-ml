{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Imports\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BO_2007_1.csv\n",
      "BO_2007_2.csv\n",
      "BO_2008_1.csv\n",
      "BO_2008_2.csv\n",
      "BO_2009_1.csv\n",
      "BO_2009_2.csv\n",
      "BO_2010_1.csv\n",
      "BO_2010_2.csv\n",
      "BO_2011_1.csv\n",
      "BO_2011_2.csv\n",
      "BO_2012_1.csv\n",
      "BO_2012_2.csv\n",
      "BO_2013_1.csv\n",
      "BO_2013_2.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Arquivos a serem carregados\n",
    "path = '../archive/'\n",
    "file_names = [f\"BO_{year}_{part}.csv\" for year in range(2007, 2014) for part in range(1, 3)]\n",
    "file_names += [\"BO_2015.csv\",'BO_2016.csv']\n",
    "\n",
    "# Carregar dados do CSV\n",
    "# pd.read_csv: Lê os arquivos CSV.\n",
    "# Amostragem (sample): Reduz a dimensão dos dados para facilitar a análise e economizar memória/processamento.\n",
    "# Concatenação (pd.concat): Junta os DataFrames amostrados em um único conjunto.\n",
    "\n",
    "def load_and_sample_files(file_names,path, sample_fraction=0.05):\n",
    "    sampled_dfs = []\n",
    "    for file in file_names:\n",
    "        print(file)\n",
    "        try:\n",
    "            df = pd.read_csv(path+file,low_memory=False)\n",
    "            \n",
    "            sampled_df = df.sample(frac=sample_fraction, random_state=1)\n",
    "            sampled_dfs.append(sampled_df)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"File {file} not found.\")\n",
    "    return pd.concat(sampled_dfs, ignore_index=True)\n",
    "\n",
    "# Arquivos no diretório\n",
    "sampled_data = load_and_sample_files(file_names,path)\n",
    "\n",
    "sampled_data.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_data = sampled_data.loc[:, ~sampled_data.columns.str.contains('^Unnamed')]\n",
    "sampled_data.dropna()\n",
    "sampled_data['IDADE_PESSOA'] = pd.to_numeric(sampled_data['IDADE_PESSOA'].str.extract('(\\d+)')[0], errors='coerce')\n",
    "# Save the sampled data into a new CSV file\n",
    "output_file = \"sampled_crime_reports.csv\"\n",
    "sampled_data['DATA_OCORRENCIA_BO'] = pd.to_datetime(sampled_data['DATA_OCORRENCIA_BO'], errors='coerce').dropna()\n",
    "sampled_data.to_csv(output_file, index=False)\n",
    "output_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_data['CIDADE'].value_counts().head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_data.columns\n",
    "[['DESCR_TIPO_PESSOA',\n",
    " 'IDADE_PESSOA',\n",
    " 'DESCR_GRAU_INSTRUCAO', \n",
    " 'RUBRICA', \n",
    " 'CIDADE',\n",
    " 'SEXO_PESSOA',\n",
    " 'COR',\n",
    " 'DESCR_PROFISSAO',\n",
    "'CONDUTA']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_crime_incidents_by_year(data):\n",
    "    data['ANO_BO'] = data['ANO_BO'].astype(int)  # Ensure the year column is of type int\n",
    "    incidents_by_year = data['ANO_BO'].value_counts().sort_index()\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    incidents_by_year.plot(kind='bar', color='skyblue')\n",
    "    plt.title('Crime Incidents by Year')\n",
    "    plt.xlabel('Year')\n",
    "    plt.ylabel('Number of Incidents')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n",
    "\n",
    "plot_crime_incidents_by_year(sampled_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_crime_distribution_by_month(data):\n",
    "    data['MES'] = data['MES'].astype(int)  # Ensure the month column is of type int\n",
    "    incidents_by_month = data['MES'].value_counts().sort_index()\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    incidents_by_month.plot(kind='bar', color='skyblue')\n",
    "    plt.title('Crime Distribution by Month')\n",
    "    plt.xlabel('Month')\n",
    "    plt.ylabel('Number of Incidents')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n",
    "\n",
    "plot_crime_distribution_by_month(sampled_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_crime_categories(data, top_n=10):\n",
    "    crime_categories = data['RUBRICA'].value_counts().head(top_n)\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    crime_categories.plot(kind='bar', color='skyblue')\n",
    "    plt.title('Top Crime Categories')\n",
    "    plt.xlabel('Crime Category')\n",
    "    plt.ylabel('Number of Incidents')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n",
    "\n",
    "plot_crime_categories(sampled_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_data[['LATITUDE', 'LONGITUDE', 'ANO_BO', 'MES']].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('sampled_crime_reports.csv').dropna()\n",
    "\n",
    "# Define the columns for clustering\n",
    "columns_for_clustering = ['DESCR_TIPO_PESSOA', 'IDADE_PESSOA', 'DESCR_GRAU_INSTRUCAO', 'RUBRICA', \n",
    "                          'CIDADE', 'SEXO_PESSOA', 'COR', 'DESCR_PROFISSAO', 'CONDUTA']\n",
    "\n",
    "# Clean and preprocess the data\n",
    "def clean_and_preprocess_data(data):\n",
    "    # Select relevant columns and drop rows with NaN values\n",
    "    data = data[['DESCR_TIPO_PESSOA',\n",
    "        'IDADE_PESSOA',\n",
    "        'DESCR_GRAU_INSTRUCAO', \n",
    "        'RUBRICA', \n",
    "        'CIDADE',\n",
    "        'SEXO_PESSOA',\n",
    "        'COR',\n",
    "        'DESCR_PROFISSAO',\n",
    "        'CONDUTA']]\n",
    "\n",
    "    # Clean the 'IDADE_PESSOA' column\n",
    "    data['IDADE_PESSOA'] = pd.to_numeric(data['IDADE_PESSOA'].str.extract('(\\d+)')[0], errors='coerce')\n",
    "\n",
    "    # Handle missing values by imputing\n",
    "    imputer = SimpleImputer(strategy='most_frequent')\n",
    "    data = pd.DataFrame(imputer.fit_transform(data), columns=columns_for_clustering)\n",
    "\n",
    "    # Convert categorical columns to numeric using LabelEncoder\n",
    "    label_encoders = {}\n",
    "    for column in columns_for_clustering:\n",
    "        if data[column].dtype == 'object':\n",
    "            label_encoders[column] = LabelEncoder()\n",
    "            data[column] = label_encoders[column].fit_transform(data[column])\n",
    "    \n",
    "    return data, label_encoders\n",
    "\n",
    "# Apply the cleaning and preprocessing\n",
    "cleaned_data, label_encoders = clean_and_preprocess_data(df)\n",
    "\n",
    "# Display the first few rows of the cleaned data\n",
    "print(cleaned_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Scale the data\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(cleaned_data)\n",
    "\n",
    "# Perform KMeans clustering\n",
    "num_clusters = 3\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "cleaned_data['Cluster'] = kmeans.fit_predict(scaled_data)\n",
    "\n",
    "# Display the first few rows with the cluster labels\n",
    "print(cleaned_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# Pair plot of selected features colored by cluster\n",
    "sns.pairplot(cleaned_data, vars=[\n",
    "        'DESCR_TIPO_PESSOA',\n",
    "        'IDADE_PESSOA',\n",
    "        'DESCR_GRAU_INSTRUCAO', \n",
    "        'RUBRICA', \n",
    "        'CIDADE',\n",
    "        'SEXO_PESSOA',\n",
    "        'COR',\n",
    "        'DESCR_PROFISSAO',\n",
    "        'CONDUTA'\n",
    "        ], hue='Cluster', palette='viridis', plot_kws={'alpha':0.6, 's':80})\n",
    "plt.suptitle('Pair Plot of Clusters', y=1.02)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the cluster labels to the original dataset\n",
    "df['Cluster'] = cleaned_data['Cluster']\n",
    "\n",
    "# Analyze the characteristics of each cluster\n",
    "def analyze_clusters(data, label_encoders):\n",
    "    cluster_analysis = data.groupby('Cluster').mean()\n",
    "    for column in label_encoders:\n",
    "        cluster_analysis[column] = cluster_analysis[column].map(lambda x: label_encoders[column].inverse_transform([int(x)])[0])\n",
    "    return cluster_analysis\n",
    "\n",
    "# Perform the cluster analysis\n",
    "cluster_analysis = analyze_clusters(cleaned_data, label_encoders)\n",
    "\n",
    "# Display the cluster analysis\n",
    "print(cluster_analysis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Convert the Cluster column to string type for plotting\n",
    "df['Cluster'] = df['Cluster'].astype(str)\n",
    "\n",
    "# Plot the distribution of clusters for each RUBRICA\n",
    "def plot_clusters_for_rubrica(data):\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    sns.countplot(data=data, x='RUBRICA', hue='Cluster', palette='viridis')\n",
    "    plt.title('Distribution of Clusters for Each RUBRICA')\n",
    "    plt.xlabel('RUBRICA')\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.legend(title='Cluster')\n",
    "    plt.show()\n",
    "\n",
    "plot_clusters_for_rubrica(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "identity_columns = [\n",
    "    'TransactionID'\n",
    "    ,'id_15'\n",
    "    ,'id_16'\n",
    "    ,'id_35'\n",
    "    ,'id_36'\n",
    "    ,'id_37'\n",
    "    ,'id_38'\n",
    "    ,'DeviceType'\n",
    "    ,'DeviceInfo'\n",
    "    ,'id_31'\n",
    "    ,'id_30'\n",
    "    ,'id_28'\n",
    "    ,'id_29'\n",
    "]\n",
    "df_identity = identity_dataset[identity_columns]\n",
    "df_identity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transaction_dataset = pd.read_csv(r'../../train_transaction.csv')\n",
    "print(transaction_dataset.columns)\n",
    "transaction_dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "- P_ and (R__) emaildomain: purchaser and recipient email domain\n",
    "- certain transactions don't need recipient, so R_emaildomain is null.\n",
    "\n",
    "The logic of our labeling is define reported chargeback on the card as fraud transaction (isFraud=1) \n",
    "    and transactions posterior to it with either user account, \n",
    "    email address or billing address directly linked to these attributes as fraud too. \n",
    "    If none of above is reported and found beyond 120 days, \n",
    "    then we define as legit transaction (isFraud=0).\n",
    "\"\"\"\n",
    "transaction_columns = [\n",
    "    'TransactionID'\n",
    "    ,'isFraud'\n",
    "    ,'TransactionDT'\n",
    "    ,'TransactionAmt'\n",
    "    ,'ProductCD'\n",
    "    ,'card1'\n",
    "    ,'card2'\n",
    "    ,'card3'\n",
    "    ,'card4'\n",
    "    ,'card5'\n",
    "    ,'addr1'\n",
    "    ,'addr2'\n",
    "    ,'P_emaildomain'\n",
    "    ,'R_emaildomain'\n",
    "    ,\n",
    "    \n",
    "]\n",
    "df_transaction = transaction_dataset[transaction_columns]\n",
    "df_transaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge = df_transaction.merge(df_identity, on='TransactionID', how='left')\n",
    "df_merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['isFraud'\n",
    "    #,'TransactionDT'\n",
    "    ,'ProductCD'\n",
    "    ,'card4'\n",
    "    ,'id_15'\n",
    "    ,'id_16'\n",
    "    ,'id_35'\n",
    "    ,'id_36'\n",
    "    ,'id_37'\n",
    "    ,'id_38'\n",
    "    ,'DeviceType'\n",
    "    ,'DeviceInfo'\n",
    "    ,'id_31'\n",
    "    ,'id_30'\n",
    "    ,'id_28'\n",
    "    ,'id_29']\n",
    "\n",
    "for column in columns:\n",
    "    print(df_merge[column].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UserID = combinação de card1, addr1, and D1\n",
    "transaction_dataset['UserID'] = transaction_dataset['card1'].astype(str) + transaction_dataset['addr1'].astype(str) + transaction_dataset['D1'].astype(str)\n",
    "transaction_dataset['UserID'] = transaction_dataset['UserID'].str.replace('nan', '0').str.replace('.', '')\n",
    "\n",
    "# Manter apenas as colunas necessárias\n",
    "transaction_dataset = transaction_dataset[['UserID','TransactionID', 'isFraud', 'TransactionDT', 'TransactionAmt', 'ProductCD', 'card4', 'card6', 'addr1', 'addr2', 'P_emaildomain']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "identity_dataset = identity_dataset[['TransactionID', 'id_30', 'id_31', 'DeviceType', 'DeviceInfo']]\n",
    "identity_dataset.rename(columns={'id_30': 'SisOp', 'id_31': 'Browser'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizar a coluna SisOp\n",
    "def normalize_sisop_column(serie):\n",
    "    os_mapping = {\n",
    "        'ios': 'iOS',\n",
    "        'android': 'Android',\n",
    "        'mac': 'Mac',\n",
    "        'windows': 'Windows',\n",
    "        'linux': 'Linux'\n",
    "    }\n",
    "\n",
    "    def normalize_os(value):\n",
    "        value = str(value).lower()\n",
    "        for key in os_mapping:\n",
    "            if key in value:\n",
    "                return os_mapping[key]\n",
    "        return 'Other'\n",
    "    \n",
    "    return serie.apply(normalize_os)\n",
    "\n",
    "identity_dataset['SisOp'] = normalize_sisop_column(identity_dataset['SisOp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizar a coluna Browser\n",
    "def normalize_browser_column(serie):\n",
    "    browser_mapping = {\n",
    "        'chrome': 'Chrome',\n",
    "        'firefox': 'Firefox',\n",
    "        'safari': 'Safari',\n",
    "        'ie': 'Internet Explorer',\n",
    "        'edge': 'Edge',\n",
    "        'samsung': 'Samsung',\n",
    "        'opera': 'Opera'\n",
    "    }\n",
    "\n",
    "    def normalize_browser(value):\n",
    "        value = str(value).lower()\n",
    "        for key in browser_mapping:\n",
    "            if key in value:\n",
    "                return browser_mapping[key]\n",
    "        return 'Other'\n",
    "    \n",
    "    return serie.apply(normalize_browser)\n",
    "\n",
    "identity_dataset['Browser'] = normalize_browser_column(identity_dataset['Browser'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contagem de transações por usuario\n",
    "user_transactions_count = transaction_dataset.groupby('UserID').size().reset_index(name='transaction_count')\n",
    "user_transactions_count.sort_values(by='transaction_count', ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser_transactions_count = identity_dataset.groupby('Browser').size().reset_index(name='transaction_count')\n",
    "browser_transactions_count.sort_values(by='transaction_count', ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge dos datasets\n",
    "full_dataset = pd.merge(transaction_dataset, identity_dataset, on='TransactionID', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tamanho dos datasets\n",
    "print('Transaction Dataset:', transaction_dataset.shape)\n",
    "print('Identity Dataset:', identity_dataset.shape)\n",
    "print('Full Dataset:', full_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Quantas transações fraudulentas existem no dataset mergeado?\\n{full_dataset.isFraud.value_counts().sort_values(ascending=False)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
